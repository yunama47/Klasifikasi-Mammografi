{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":8774437,"sourceType":"datasetVersion","datasetId":5076463}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output\n!rm -rf ./*\n!pip install keras==2.15 \n!pip install albumentations\n!pip install keras-tuner\nclear_output()\nprint(\"-- dependency installs completed --\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-26T12:50:36.694823Z","iopub.execute_input":"2024-06-26T12:50:36.695649Z","iopub.status.idle":"2024-06-26T12:50:47.427852Z","shell.execute_reply.started":"2024-06-26T12:50:36.695611Z","shell.execute_reply":"2024-06-26T12:50:47.427073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports and some things","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport cv2\nimport keras\nimport keras_tuner\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\ndef seed_everything(seed=911):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(911)\n\nclass D:\n    '''class that contain all the parameters'''\n    #param for dataset\n    image_size = (512,288)\n    batch_size = 4\n    images_dir = '/kaggle/input/multi-view-dataset-v2/images'\n    shuffle_buffer = 3000\n    splits = [f'fold-{i}' for i in range(5)]\n    views = ['Examined','Aux']\n    \n    #param for ConvNeXt multi-view model\n    model_var = 'convnext_small'\n    drop_path_rate = 0.2123456789\n    dropout_rate   = 0.5123456789\n    pooling ='w_avg'\n    fusion_stage = 3\n    fusion_index = 1\n    fc_layers_depth = 3\n    fc_layers_dims = 256\n    \n    #param for training\n    epochs = 60\n    loss_fn = 'categorical_crossentropy'\n    optimizer = lambda: keras.optimizers.Adam(1e-5, use_ema=True)\n    metrics = lambda : [\n        keras.metrics.CategoricalAccuracy(name='accuracy'),\n        keras.metrics.F1Score(average='macro',name='macro_F1')\n    ]\ntry :\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.TPUStrategy(resolver)\n    D.TPU = True\n    D.batch_size *= 8\nexcept ValueError:\n    D.TPU = False\n    strategy = tf.distribute.MirroredStrategy()\nclear_output()\nprint(\"-- versions --\")\nprint(\"tf :\",tf.__version__)\nprint(\"keras :\",keras.__version__)\nprint(\"keras_tuner :\",keras_tuner.__version__)\nprint(\"albumentations :\",A.__version__)\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-06-26T12:50:47.429505Z","iopub.execute_input":"2024-06-26T12:50:47.429786Z","iopub.status.idle":"2024-06-26T12:50:59.929463Z","shell.execute_reply.started":"2024-06-26T12:50:47.429759Z","shell.execute_reply":"2024-06-26T12:50:59.928616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# loading dataset","metadata":{}},{"cell_type":"code","source":"from random import choice\n\nTOUT = \"float32\"\n\n@tf.numpy_function(Tout=TOUT)\ndef cutout_mask(shape, n_holes, min_max_height, min_max_width):\n    h, w, c = shape\n    mask = np.ones(shape, dtype=np.float32)\n    for _ in range(n_holes):\n        y = np.random.randint(0, h)\n        x = np.random.randint(0, w)\n        hmax, hmin = min_max_height\n        wmax, wmin = min_max_width\n        cutout_height = np.random.randint( hmax, hmin)\n        cutout_width = np.random.randint( wmax, wmin)\n        y1 = np.clip(y - cutout_height // 2, 0, h)\n        y2 = np.clip(y + cutout_height // 2, 0, h)\n        x1 = np.clip(x - cutout_width // 2, 0, w)\n        x2 = np.clip(x + cutout_width // 2, 0, w)\n        mask[y1: y2, x1: x2, :] = 0\n    mask = tf.cast(mask, TOUT) \n    return mask\n\nclass CutoutLayer(keras.layers.Layer):\n    def __init__(self, min_max_holes=(None,None), min_max_height=(None, None), min_max_width=(None, None),p=1, **kwargs):\n        super().__init__(**kwargs)\n        min_holes, max_holes = min_max_holes\n        self.n_holes = tf.random.uniform((), min_holes, max_holes, dtype=tf.int32)\n        self.min_max_height = min_max_height\n        self.min_max_width = min_max_width\n        self.p = p\n\n    def call(self, inputs):\n        h = tf.shape(inputs)[-3]\n        w = tf.shape(inputs)[-2]\n        c = tf.shape(inputs)[-1]\n        mask =  cutout_mask((h, w, c), self.n_holes, self.min_max_height, self.min_max_width)\n        outputs = inputs * mask\n        return outputs\n\n@tf.numpy_function(Tout=TOUT)\ndef shift_scale_rotate(image, scale_limit, rotate_limit, shift_limit_x, shift_limit_y):\n    image = image.astype(\"float32\")\n    ssr = A.ShiftScaleRotate(shift_limit=(shift_limit_x, shift_limit_x),\n                             scale_limit=(scale_limit, scale_limit), \n                             rotate_limit=(rotate_limit, rotate_limit), \n                             interpolation=cv2.INTER_LINEAR,\n                             border_mode=cv2.BORDER_CONSTANT, \n                             value=0, mask_value=0,\n                             shift_limit_x=(shift_limit_x, shift_limit_x),\n                             shift_limit_y=(shift_limit_y, shift_limit_y), \n                             rotate_method='largest_box', p=1.0)\n    augmented = ssr(image=image)[\"image\"]\n    augmented = tf.cast(augmented, TOUT)\n    return augmented\n\nclass ShiftScaleRotate(keras.layers.Layer):\n    def __init__(self, scale, rotate, shift_x, shift_y,  **kwargs):\n        super().__init__(**kwargs)\n        self.scale = scale\n        self.rotate = rotate\n        self.shift_x = shift_x\n        self.shift_y = shift_y\n\n    def call(self, inputs):\n        outputs = shift_scale_rotate(inputs, self.scale,self.rotate,self.shift_x,self.shift_y)\n        return outputs\n\nclass BrightnessContrastLayer(keras.layers.Layer):\n    def __init__(self, contrast_factor = None,brightness_factor=None, p=1, **kwargs):\n        super().__init__(**kwargs)\n        self.contrast_factor = contrast_factor\n        self.brightness_factor = brightness_factor\n        self.p = p\n\n    def call(self, inputs):\n        outputs = tf.image.adjust_contrast(inputs, self.contrast_factor)\n        outputs = tf.experimental.numpy.clip(outputs, 0, 255)\n        outputs = tf.image.adjust_brightness(outputs, self.brightness_factor)\n        outputs = tf.experimental.numpy.clip(outputs, 0, 255)\n        return outputs\n\n\ndef augment_img(img_dict):\n    if tf.random.uniform((),0,1) < 0.45: #rate for overal augmentations\n        return img_dict\n    Ex_image = img_dict['Examined']\n    Aux_image = img_dict['Aux']\n\n    cutout = CutoutLayer(min_max_holes=(1,8), \n                         min_max_height=(int(0.05 * D.image_size[0]), int(0.15 * D.image_size[0])), \n                         min_max_width=(int(0.05 * D.image_size[1]), int(0.15 * D.image_size[1])),\n                        )\n    rotate_zoom = ShiftScaleRotate(scale= tf.random.uniform((),-0.15,0.2), \n                                   rotate= tf.random.uniform((),-20,20), \n                                   shift_x=tf.random.uniform((),-0.20,0.20), \n                                   shift_y=tf.random.uniform((),-0.1,0.1))\n    brightness_contrast = BrightnessContrastLayer(contrast_factor=tf.random.uniform((),0.8, 1.5), \n                                                  brightness_factor=tf.random.uniform((),-0.1, 0.2))\n    augmentations = {\n                     cutout : 0.4,\n                     tf.image.flip_left_right :0.5,\n                     tf.image.flip_up_down :0.5,\n                     rotate_zoom : 0.4,\n                     brightness_contrast :0.5\n                    }\n\n    for aug, p in augmentations.items():\n        if tf.random.uniform((), 0, 1) < p:\n            Ex_image = aug(Ex_image)\n            Aux_image = aug(Aux_image)\n        else:\n            Ex_image = Ex_image\n            Aux_image = Aux_image\n\n    Ex_image = tf.reshape(Ex_image, (D.image_size[0],D.image_size[1], 3))\n    Aux_image = tf.reshape(Aux_image, (D.image_size[0],D.image_size[1], 3))\n\n    if tf.random.uniform((),0,1) < 0.5: # swap view position\n        img_dict['Examined']= Aux_image\n        img_dict['Aux'] = Ex_image\n    else:\n        img_dict['Examined'] = Ex_image\n        img_dict['Aux']= Aux_image\n        \n    if tf.random.uniform((),0,1) < 0.1: # remove auxiliary view\n        img_dict['Aux'] = tf.zeros_like(Aux_image)\n#     else:\n#         img_dict['Aux']= Aux_image\n        \n    return img_dict\n\n0","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-26T12:50:59.930889Z","iopub.execute_input":"2024-06-26T12:50:59.931190Z","iopub.status.idle":"2024-06-26T12:50:59.963907Z","shell.execute_reply.started":"2024-06-26T12:50:59.931160Z","shell.execute_reply":"2024-06-26T12:50:59.963148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_label(file_path):\n    path_parts = tf.strings.split(file_path, os.path.sep)\n    filename_parts = tf.strings.split(path_parts[-1], \"_\")\n    label = tf.strings.to_number(filename_parts[-3],out_type=tf.dtypes.int32)\n\n    return tf.one_hot(label-1,5)\n\ndef decode_img(img):\n    img = tf.io.decode_png(img, channels=3)\n    # Resize the image to the desired size\n    return tf.image.resize(img, D.image_size)\n\ndef process_image(file_path):\n    # Load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return tf.cast(img, TOUT)\n\ndatasets_dict = {\n    f'fold-{i}':[\n        {\n            'Examined':None,\n            'Aux':None,\n        },\n        [] #for label\n    ] for i in range (5)\n}\nview_map = {\"Examined\":\"CC\", \"Aux\":\"MLO\"}\n#https://www.tensorflow.org/tutorials/load_data/images?hl=en#using_tfdata_for_finer_control\nfor split in D.splits:\n    for view in D.views:\n        datasets_dict[split][0][view] = tf.data.Dataset.list_files(f\"{D.images_dir}/*_{view_map[view]}_{split}.png\",shuffle=False)\n\nfor view in D.views:\n    total = 0\n    print(f\"{view} dataset\".center(40, \"-\"))\n    for split in D.splits:\n        ds = datasets_dict[split][0][view]\n        fold_cardinality = ds.cardinality().numpy()\n        total += fold_cardinality\n        print(f'{view}-{split} cardinality :', fold_cardinality)\n        print('examples :')\n        for f in ds.take(2):\n            print(f.numpy())\n        print()\n    print(\"total = \",total)\n\n        \nfor split in D.splits:\n    datasets_dict[split][1] = datasets_dict[split][0][view].map(get_label, num_parallel_calls=tf.data.AUTOTUNE)\n    for view in D.views:\n        datasets_dict[split][0][view] = datasets_dict[split][0][view].map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n\nprint(\"\".center(40, \"-\"))\nfor image in datasets_dict[D.splits[0]][0][D.views[0]].take(1):\n    print(\"image dtype =\", image.dtype) \nprint(\"unique labels :\\n\",np.unique(list(datasets_dict[D.splits[0]][1].as_numpy_iterator()), axis=0))\n\nfold_ds = []\nfor split in D.splits:\n    fold_i = tf.data.Dataset.zip(tuple(datasets_dict[split]))\n    fold_ds.append(fold_i)\n\ndel datasets_dict\n\ndef prepare_ds(ds, training=True):\n    ds = ds.cache()\n    if training:\n        ds = ds.repeat(1)\n        ds = ds.shuffle(buffer_size=D.shuffle_buffer*2)\n        ds = ds.map(lambda x, y: (augment_img(x), y),\n                    num_parallel_calls=tf.data.AUTOTUNE)\n    ds = ds.batch(D.batch_size)\n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return ds","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-26T12:50:59.965656Z","iopub.execute_input":"2024-06-26T12:50:59.966203Z","iopub.status.idle":"2024-06-26T12:51:01.386868Z","shell.execute_reply.started":"2024-06-26T12:50:59.966170Z","shell.execute_reply":"2024-06-26T12:51:01.385898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# visualization","metadata":{}},{"cell_type":"code","source":"from tensorflow.errors import InvalidArgumentError\n\nviz_ds = prepare_ds(fold_ds[0], training=True)\nprint(repr(viz_ds))\nbatch = next(iter(viz_ds))\nEx_image_batch = batch[0]['Examined']\nAux_image_batch = batch[0]['Aux']\nlabel_batch = batch[1]\n\nncol = 4\nnrow = 4\nbar_color = [0, 255, 0]\nbar_thickness = 5\nbar = np.array([[bar_color for i in range(bar_thickness)]\n                           for j in range(D.image_size[0])])\nrandom_start = 0 if D.batch_size < 21 else np.random.randint(0,D.batch_size-nrow*ncol)\nplt.figure(figsize=(20, 5*nrow))\nfor i in range(nrow*ncol):\n    index = random_start+i\n    try:\n        EX = Ex_image_batch[index].numpy().astype(\"uint8\")\n        AUX = Aux_image_batch[index].numpy().astype(\"uint8\")\n    except InvalidArgumentError:\n        break\n    ax = plt.subplot(nrow, ncol, i + 1)\n    image = np.hstack((EX,bar,AUX))\n    plt.imshow(image)\n    label = label_batch[index]\n    plt.title(f\"({index})\\nBI-RADS {np.argmax(label)+1}\\nExamined|Aux\")\n    plt.axis(\"off\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-26T12:51:01.388006Z","iopub.execute_input":"2024-06-26T12:51:01.388353Z","iopub.status.idle":"2024-06-26T12:51:05.325260Z","shell.execute_reply.started":"2024-06-26T12:51:01.388319Z","shell.execute_reply":"2024-06-26T12:51:05.324349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Learning rate schedule for TPU, GPU and CPU.\n# Using an LR ramp up because fine-tuning a pre-trained model.\n# Starting with a high LR would break the pre-trained weights.\ndef get_lr_callback(plot_schedule=False, EPOCHS=D.epochs):\n    LR_START = 0.00001\n    LR_MAX   = 0.00005 * strategy.num_replicas_in_sync\n    LR_MIN   = 0.00001\n    LR_RAMPUP_EPOCHS = round(EPOCHS*0.25)\n    LR_SUSTAIN_EPOCHS = 0\n    LR_EXP_DECAY = .9\n\n    def lrfn(epoch):\n        if epoch < LR_RAMPUP_EPOCHS:\n            lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n        elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n            lr = LR_MAX\n        else:\n            lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n        return lr\n    \n    if plot_schedule:\n        rng = [i for i in range(25 if EPOCHS < 25 else EPOCHS)]\n        y = [lrfn(x) for x in rng]\n        plt.plot(rng, y)\n\n    return keras.callbacks.LearningRateScheduler(lrfn, verbose=0)\nget_lr_callback(plot_schedule=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T12:51:05.326590Z","iopub.execute_input":"2024-06-26T12:51:05.326881Z","iopub.status.idle":"2024-06-26T12:51:05.494164Z","shell.execute_reply.started":"2024-06-26T12:51:05.326852Z","shell.execute_reply":"2024-06-26T12:51:05.493415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@keras.saving.register_keras_serializable(\"CustomLayers\", name=\"custom_global_pooling\")\nclass GlobalPooling2D(keras.layers.Layer):\n    '''\n    modified from : https://github.com/csvance/keras-global-weighted-pooling/blob/master/gwp.py#L51\n    reference : https://arxiv.org/abs/1809.08264\n    '''\n    def __init__(self,pool_func:str, activation:str='linear', **kwargs):\n        super().__init__(**kwargs)\n        assert pool_func in ['max', 'avg','w_max', 'w_avg'], \"one of : 'max', 'avg','w_max', 'w_avg'\"\n        self.act = activation\n        self.pool_func = pool_func\n\n    def build(self, input_shape):\n        if \"w_\" in self.pool_func:\n            self.kernel = self.add_weight(name='kernel',\n                                          shape=(input_shape[1], input_shape[2], 1),\n                                          initializer='ones',\n                                          trainable=True)\n            self.bias = self.add_weight(name='bias',\n                                        shape=(1,),\n                                        initializer='zeros',\n                                        trainable=True)\n        else:\n            pass\n        super().build(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[3],\n\n    def call(self, x):\n        if \"w_avg\" == self.pool_func:\n            z = tf.reduce_mean(x*self.kernel, axis=(1, 2)) + self.bias\n        elif \"w_max\" == self.pool_func:\n            z = tf.reduce_max(x*self.kernel, axis=(1, 2)) + self.bias\n        elif \"max\" == self.pool_func:\n            z = tf.reduce_max(x, axis=(1, 2))\n        elif \"avg\" == self.pool_func:\n            z = tf.reduce_mean(x, axis=(1, 2))\n        x = keras.layers.Activation(self.act)(z)\n        return x\n\n@keras.saving.register_keras_serializable(\"CustomLayers\", name=\"stochastic_depth\")\nclass StochasticDepth(keras.layers.Layer):\n    \"\"\"\n    source : https://github.com/keras-team/keras/blob/v3.3.3/keras/src/applications/convnext.py#L140\n    \"\"\"\n    def __init__(self, drop_path_rate, **kwargs):\n        super().__init__(**kwargs)\n        self.drop_path_rate = drop_path_rate\n\n    def call(self, x, training=None):\n        if training:\n            keep_prob = 1 - self.drop_path_rate\n            shape = (tf.shape(x)[0],) + (1,) * (len(x.shape) - 1)\n            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n            random_tensor = tf.floor(random_tensor)\n            random_tensor = tf.cast(random_tensor, TOUT)\n            return (x / keep_prob) * random_tensor\n        return x\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"drop_path_rate\": self.drop_path_rate})\n        return config\n\n@keras.saving.register_keras_serializable(\"CustomLayers\", name=\"layer_scale\")\nclass LayerScale(keras.layers.Layer):\n    \"\"\"\n    source : https://github.com/keras-team/keras/blob/v3.3.3/keras/src/applications/convnext.py#L177\n    \"\"\"\n\n    def __init__(self, init_values, projection_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.init_values = init_values\n        self.projection_dim = projection_dim\n\n    def build(self, _):\n        self.gamma = self.add_weight(\n            name='gamma',\n            shape=(self.projection_dim,),\n            initializer=keras.initializers.Constant(self.init_values),\n            trainable=True,\n        )\n\n    def call(self, x):\n        return x * self.gamma\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"init_values\": self.init_values,\n                \"projection_dim\": self.projection_dim,\n            }\n        )\n        return config\n\n    \nkeras.saving.get_custom_objects()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-26T12:51:05.495084Z","iopub.execute_input":"2024-06-26T12:51:05.495346Z","iopub.status.idle":"2024-06-26T12:51:05.515115Z","shell.execute_reply.started":"2024-06-26T12:51:05.495322Z","shell.execute_reply":"2024-06-26T12:51:05.514391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.applications import (\n    ConvNeXtTiny,\n    ConvNeXtSmall,\n    ConvNeXtBase,\n    ConvNeXtLarge,\n    ConvNeXtXLarge,\n)\n\nconvnext_varians_map = {\n    \"convnext_tiny\" : ConvNeXtTiny,\n    \"convnext_small\": ConvNeXtSmall,\n    \"convnext_base\" : ConvNeXtBase,\n    \"convnext_large\": ConvNeXtLarge,\n    \"convnext_xlarge\": ConvNeXtXLarge,\n}\n\nconvnext_dims_depth_map = {\n    \"convnext_tiny\" : ([96, 192, 384, 768], [3, 3, 9, 3]),\n    \"convnext_small\": ([96, 192, 384, 768], [3, 3, 27, 3]),\n    \"convnext_base\" : ([128, 256, 512, 1024], [3, 3, 27, 3]),\n    \"convnext_large\": ([192, 384, 768, 1536], [3, 3, 27, 3]),\n    \"convnext_xlarge\":([256, 512, 1024, 2048], [3, 3, 27, 3]),\n}\n\ndef load_convnext(variant=D.model_var):\n    convnext = convnext_varians_map[variant](\n        include_top=False,\n        weights=\"imagenet\",\n        input_shape=(D.image_size[0],D.image_size[1],3),\n    )\n    return convnext\n\ndef get_dims_depth(variant=D.model_var):\n    return convnext_dims_depth_map[variant]\n    \ndef ConvNext_Block(x, dim,\n                   stage=0,\n                   block=0,\n                   pretrained=None,\n                   layer_scale_init_value=1e-6,\n                   drop_path_rate=None,\n                   variant='convnext_small', \n                   name=''\n                  ):\n    prew = None\n    if pretrained:\n        prew = {\n              'conv':[\n                  pretrained[f'{variant}_stage_{stage}_block_{block}_depthwise_conv.kernel'].numpy(),\n                  pretrained[f'{variant}_stage_{stage}_block_{block}_depthwise_conv.bias'].numpy(),\n                  ],\n              'lnorm':[\n                  pretrained[f'{variant}_stage_{stage}_block_{block}_layernorm.gamma'].numpy(),\n                  pretrained[f'{variant}_stage_{stage}_block_{block}_layernorm.beta'].numpy(),\n                  ],\n              'pointwise1':[\n                  pretrained[f'{variant}_stage_{stage}_block_{block}_pointwise_conv_1.kernel'].numpy(),\n                  pretrained[f'{variant}_stage_{stage}_block_{block}_pointwise_conv_1.bias'].numpy(),\n                  ],\n              'pointwise2':[\n                  pretrained[f'{variant}_stage_{stage}_block_{block}_pointwise_conv_2.kernel'].numpy(),\n                  pretrained[f'{variant}_stage_{stage}_block_{block}_pointwise_conv_2.bias'].numpy(),\n                  ],\n              'layer_scale' : [pretrained[f'{variant}_stage_{stage}_block_{block}_layer_scale.gamma']]\n          }\n  \n\n    depthwise_convolution = keras.layers.Conv2D(dim, \n                                                   kernel_size=7, \n                                                   padding=\"same\", \n                                                   groups=dim, \n                                                   name = f'{name}_{stage}-{block}_depthwise_conv'\n                                                  )\n    layer_normalization = keras.layers.LayerNormalization(epsilon=1e-6, name=f'{name}_{stage}-{block}_layernorm')\n    pointwise_convolution_1 = keras.layers.Dense(4 * dim, name=f'{name}_{stage}-{block}_pointwise_conv1')\n    GELU = keras.layers.Activation(\"gelu\", name=f'{name}_{stage}-{block}_gelu')\n    pointwise_convolution_2 = keras.layers.Dense(dim, name=f'{name}_{stage}-{block}_pointwise_conv2')\n    add = keras.layers.Add(name=f'{name}_{stage}-{block}_output')\n    layer_scale = LayerScale(layer_scale_init_value, projection_dim=dim, name=f'{name}_{stage}-{block}_layer_scale')\n\n    o = depthwise_convolution(x)\n    o = layer_normalization(o)\n    o = pointwise_convolution_1(o)\n    o = GELU(o)\n    o = pointwise_convolution_2(o)\n    o = layer_scale(o)\n    if drop_path_rate is not None:\n        o = StochasticDepth(\n            drop_path_rate, name=f'{name}_{stage}-{block}_stochastic_depth'\n        )(o)\n    else:\n        o = keras.layers.Activation(\"linear\", name=f'{name}_{stage}-{block}_identity')(o)\n    if prew:\n        depthwise_convolution.set_weights(prew['conv'])\n        layer_normalization.set_weights(prew['lnorm'])\n        pointwise_convolution_1.set_weights(prew['pointwise1'])\n        pointwise_convolution_2.set_weights(prew['pointwise2'])\n        layer_scale.set_weights(prew['layer_scale'])\n\n    return add([o, x])\n\n\ndef patchify_stem(x, \n                  dim=96, \n                  pretrained=None, \n                  name='single', \n                  variant='convnext_small'\n                 ):\n    conv = keras.layers.Conv2D(dim, \n                              kernel_size=4, \n                              strides=4,\n                              name=f'{name}_stem_conv')\n    layer_norm = keras.layers.LayerNormalization(epsilon=1e-6,name=f'{name}_stem_layer_norm')\n    o = conv(x)\n    o = layer_norm(o)\n    if pretrained:\n        prew = pretrained\n        conv_w = [\n            prew[f'{variant}_stem.layer_with_weights-0.kernel'].numpy(),\n            prew[f'{variant}_stem.layer_with_weights-0.bias'].numpy(),\n            ]\n        layer_norm_w = [\n            prew[f'{variant}_stem.layer_with_weights-1.gamma'].numpy(),\n            prew[f'{variant}_stem.layer_with_weights-1.beta'].numpy(),\n        ]\n        conv.set_weights(conv_w)\n        layer_norm.set_weights(layer_norm_w)\n\n    return o\n\ndef spatial_downsampling(x, \n                         stage, \n                         dim,\n                         pretrained=None, \n                         kernel_size=2,\n                         stride=2,\n                         name='single', \n                         variant='convnext_small'\n                        ):\n    layer_norm = keras.layers.LayerNormalization(epsilon=1e-6,name=f'{name}_downsampling_{stage}_layer_norm')\n    conv = keras.layers.Conv2D(dim, \n                              kernel_size=kernel_size, \n                              strides=stride,\n                              name=f'{name}_downsampling_{stage}_conv'\n                              )\n    o = layer_norm(x)\n    o = conv(o)\n    if pretrained:\n        prew = pretrained\n        layer_norm_w = [\n            prew[f'{variant}_downsampling_block_{stage-1}.layer_with_weights-0.gamma'].numpy(),\n            prew[f'{variant}_downsampling_block_{stage-1}.layer_with_weights-0.beta'].numpy(),\n        ]\n        conv_w = [\n            prew[f'{variant}_downsampling_block_{stage-1}.layer_with_weights-1.kernel'].numpy(),\n            prew[f'{variant}_downsampling_block_{stage-1}.layer_with_weights-1.bias'].numpy(),\n        ]\n        conv.set_weights(conv_w)\n        layer_norm.set_weights(layer_norm_w)\n    return o\n\ndef ConvNext_Stage(x, \n                   dim, \n                   depth, \n                   stage,  \n                   pretrained=None, \n                   layer_scale_init_value=1e-6, \n                   depth_drop_rates=None,\n                   name='stage',\n                   variant='convnext_small'\n                  ):\n    o = x\n    if depth_drop_rates is None:\n        depth_drop_rates = np.zeros(depth)\n    for j in range(depth):\n        o = ConvNext_Block(o,\n                           dim=dim, \n                           pretrained=pretrained, \n                           stage=stage, block=j, \n                           layer_scale_init_value=layer_scale_init_value,\n                           variant=variant,\n                           drop_path_rate=depth_drop_rates[j],\n                           name=name\n                          )\n    return o\n\ndef ConvNext_stage_and_downsampling(x,\n                                    dim, \n                                    depth, \n                                    i, \n                                    pretrained, \n                                    depth_drop_rates=None,\n                                    view='single', \n                                    variant='convnext_small'\n                                   ):\n    if i == 0:\n        x = keras.layers.Normalization(\n            mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n            variance=[\n                (0.229 * 255) ** 2,\n                (0.224 * 255) ** 2,\n                (0.225 * 255) ** 2,\n            ],\n            name=f'{variant}_{view}_norm'\n        )(x)\n        x = patchify_stem(pretrained=pretrained,\n                        dim=dim,\n                        variant=variant,\n                        name=f'{variant}_{view}',x=x)\n    else:\n        x = spatial_downsampling(pretrained=pretrained,\n                                  dim=dim,\n                                  stage=i,\n                                  kernel_size=2,\n                                  stride=2,\n                                  variant=variant,\n                                  name=f'{variant}_{view}',x=x)\n\n    x = ConvNext_Stage(x=x,\n                       dim=dim, \n                       depth=depth, \n                       stage=i, \n                       pretrained=pretrained, \n                       depth_drop_rates=depth_drop_rates,\n                       name=f'{variant}_{view}_stage',\n                       variant=variant,)\n    return x\n\n\ndef multi_view_fusion_stage(pre_fusion, \n                            i, \n                            pretrained_weights=None, \n                            dims=None,\n                            depths=None, \n                            depth_drop_rates=None,\n                            fusion_block_index=0,\n                            model_var=D.model_var\n                           ):\n    for view, x in pre_fusion.items():\n        x = spatial_downsampling(pretrained=pretrained_weights,\n                                  dim=dims[i],\n                                  stage=i,\n                                  kernel_size=2,\n                                  stride=2,\n                                  variant=model_var,\n                                  name=f\"{model_var}_{view}_fusion_downsampling\",\n                                  x=x\n                                )\n        pre_fusion[view] = x\n    x_dual_skip = pre_fusion.copy()\n    if depth_drop_rates is None:\n        depth_drop_rates = np.zeros(depths[i])\n    # stage iteration here\n    for j in range(depths[i]):\n        if j < fusion_block_index:\n            for view, x in pre_fusion.items():\n                x = ConvNext_Block(x, \n                                  dims[i],stage=i,\n                                  block=j,\n                                  pretrained=pretrained_weights, \n                                  drop_path_rate=depth_drop_rates[j],\n                                  variant=model_var,\n                                  name=f'{model_var}_{view}_fusion_stage')\n                pre_fusion[view] = x_dual_skip[view] = x\n            continue\n        elif j == fusion_block_index:\n            x = keras.layers.Average(name=f'{model_var}_fusion_merge')(list(pre_fusion.values()))\n            x = ConvNext_Block(x, \n                              dims[i],stage=i,\n                              block=j,\n                              pretrained=pretrained_weights, \n                              drop_path_rate=depth_drop_rates[j],\n                              variant=model_var,\n                              name=f'{model_var}_{view}_post-fusion_stage')\n            x = keras.layers.Add(name=\"merge_fused_and_examined_skip\")([x, x_dual_skip[\"Examined\"]])\n            continue\n        elif j > fusion_block_index:\n            x = ConvNext_Block(x, \n                              dims[i],stage=i,\n                              block=j,\n                              pretrained=pretrained_weights, \n                              drop_path_rate=depth_drop_rates[j],\n                              variant=model_var,\n                              name=f'{model_var}_{view}_post-fusion_stage')\n    return x\n0","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-26T12:51:05.516394Z","iopub.execute_input":"2024-06-26T12:51:05.517120Z","iopub.status.idle":"2024-06-26T12:51:05.559356Z","shell.execute_reply.started":"2024-06-26T12:51:05.517052Z","shell.execute_reply":"2024-06-26T12:51:05.558659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_inputs():\n    inputs = {\n                \"Examined\" : keras.Input(shape=[*D.image_size,3], name='Examined', dtype=tf.float32),\n                \"Aux\" : keras.Input(shape=[*D.image_size,3], name='Aux', dtype=tf.float32)\n             }\n    return inputs\n\ndef model_compile(model):\n    model.compile(\n        loss=D.loss_fn,\n        optimizer=D.optimizer(),\n        metrics=D.metrics(),\n        jit_compile=True if D.TPU else False,\n    )\n    return model\n\ndef model_fit(model, fold=1, initial_epoch=0,epoch=D.epochs, verbose = 2, initial_val_F1=0.4):\n    validation_ds = prepare_ds(fold_ds[fold], training=False)\n    train_fold = fold_ds[:fold] + fold_ds[fold + 1:]\n    train_ds = train_fold[0]\n    for i in range(1, len(train_fold)):\n        train_ds = train_ds.concatenate(train_fold[i])\n    train_ds = prepare_ds(train_ds, training=True)\n    ckpt_callback = keras.callbacks.ModelCheckpoint(\n                        filepath=f'checkpoints/{D.model_var}_fold-{fold}_best.weights.h5',\n                        monitor='val_macro_F1',\n                        verbose=2,\n                        mode='max',\n                        save_best_only=True,\n                        save_weights_only=True,\n                        initial_value_threshold=initial_val_F1,\n                       )\n    history = model.fit(\n        train_ds,\n        initial_epoch=initial_epoch,\n        epochs = epoch,\n        verbose = verbose,\n        validation_data=validation_ds,\n        callbacks=[ckpt_callback, get_lr_callback()]\n    )\n    return history, ckpt_callback.best\n\ndef create_model(model_var=D.model_var, \n                 fusion_stage=2, \n                 fusion_block_index=1,\n                 fc_layers_depth=1, \n                 fc_layers_dims=512, \n                 drop_path_rate=D.dropout_rate,\n                 drop_out_rate=0.5,\n                 pooling='avg',\n                 pretrained_weights=None,\n                ):\n    \n    inputs = get_inputs()\n    pre_fusion = {key:value for key, value in inputs.items()}\n    dims , depths = get_dims_depth(model_var)\n    depth_drop_rates = np.linspace(0, drop_path_rate, sum(depths), dtype=float)\n    blocks_passed = 0\n    for i in range(len(dims)):\n        current_stage_depth_drop_rates = depth_drop_rates[blocks_passed:blocks_passed+depths[i]]\n        blocks_passed+=depths[i]\n        if i < fusion_stage:\n            for key in D.views:\n                pre_fusion[key] = ConvNext_stage_and_downsampling(pre_fusion[key], \n                                                                   dims[i], \n                                                                   depths[i], i, \n                                                                   pretrained_weights, \n                                                                   variant=model_var, \n                                                                   depth_drop_rates=current_stage_depth_drop_rates,\n                                                                   view=key)\n            continue\n        if i == fusion_stage:\n            x = multi_view_fusion_stage(pre_fusion, \n                                        i,\n                                        pretrained_weights=pretrained_weights, \n                                        dims=dims,\n                                        depths=depths, \n                                        depth_drop_rates=current_stage_depth_drop_rates,\n                                        fusion_block_index=fusion_block_index,\n                                        model_var=model_var,\n                                       )\n            continue\n        x = ConvNext_stage_and_downsampling(x, dims[i], depths[i], i, \n                                        pretrained_weights,view=\"fine\", \n                                        depth_drop_rates=current_stage_depth_drop_rates, variant=model_var)\n    x = GlobalPooling2D(pooling, name=f'{model_var}_global_pooling')(x)\n    LN1 = keras.layers.LayerNormalization(epsilon=1e-6, name=f'{model_var}_pre_FC_ln')\n    x = LN1(x)\n    if pretrained_weights:\n        LN1.set_weights([\n            pretrained_weights['layer_normalization.gamma'].numpy(),\n            pretrained_weights['layer_normalization.beta'].numpy(),\n        ])\n    x = keras.layers.Dropout(drop_out_rate)(x)\n    for i in range(fc_layers_depth):\n        x = keras.layers.Dense(fc_layers_dims, activation='gelu', name=f'{model_var}_cls_{i}')(x)\n    output = keras.layers.Dense(5, activation='softmax',dtype='float32' ,name=f'{model_var}_output')(x)\n    model = model_compile(keras.src.models.Functional(inputs, output, name=f'{model_var}_mammo_multi_view'))\n    return model\n\ndef plot_history_metrics(history, model_name=\"convnext\"):\n    loss = history.history.pop('loss')\n    val_loss = history.history.pop('val_loss')\n    history.history.pop('lr')\n    epochs = range(D.epochs)\n    plt.plot(epochs, loss, 'r', label='Training Loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n    plt.legend()\n    plt.title(f'Training and validation loss | {model_name}')\n    plt.figure()\n    for key, values in history.history.items():\n        plt.plot(epochs, values, label=key)\n    plt.title(f'Training and validation metrics | {model_name}')\n    plt.legend()\n    plt.show()\n    \ndef plot_history_metrics_for_multi_model(histories:dict, metric_to_plot=\"loss\"):\n    for fold, history in histories.items():\n        metric_history = history.history.get(metric_to_plot)\n        epochs = range(D.epochs)\n        plt.plot(epochs, metric_history, label=fold)\n    plt.legend()\n    plt.title(f'{metric_to_plot} history')\n    plt.show()\n0","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-26T12:51:05.560303Z","iopub.execute_input":"2024-06-26T12:51:05.560560Z","iopub.status.idle":"2024-06-26T12:51:05.584630Z","shell.execute_reply.started":"2024-06-26T12:51:05.560532Z","shell.execute_reply":"2024-06-26T12:51:05.583923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_weights = load_convnext(D.model_var).get_weight_paths()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-26T12:51:05.587157Z","iopub.execute_input":"2024-06-26T12:51:05.587444Z","iopub.status.idle":"2024-06-26T12:51:10.330745Z","shell.execute_reply.started":"2024-06-26T12:51:05.587415Z","shell.execute_reply":"2024-06-26T12:51:10.329902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = create_model(model_var=D.model_var, \n#                      fusion_stage=2, \n#                      fusion_block_index=9,\n#                      fc_layers_depth=D.fc_layers_depth, \n#                      fc_layers_dims=D.fc_layers_dims, \n#                      drop_path_rate=D.dropout_rate,\n#                      pooling=D.pooling,\n#                      pretrained_weights=pretrained_weights,\n#                     )\n\n# keras.utils.plot_model(model,show_shapes=True, \n#                        rankdir=\"TB\", dpi=128,)\n# model.summary()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-26T12:51:10.331714Z","iopub.execute_input":"2024-06-26T12:51:10.331957Z","iopub.status.idle":"2024-06-26T12:51:10.335553Z","shell.execute_reply.started":"2024-06-26T12:51:10.331933Z","shell.execute_reply":"2024-06-26T12:51:10.334846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.get_layer(f'{D.model_var}_Examined_stage_{1}-{2}_stochastic_depth').get_config()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T12:51:10.336415Z","iopub.execute_input":"2024-06-26T12:51:10.336689Z","iopub.status.idle":"2024-06-26T12:51:10.350367Z","shell.execute_reply.started":"2024-06-26T12:51:10.336662Z","shell.execute_reply":"2024-06-26T12:51:10.349678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -rf keras-tuner\n# hyperparams = {\n#     \"pooling\": ['w_avg'],\n#     \"fusion_stage\" : [1, 2, 3],\n#     \"fusion_block_index\" : [0, 1, 2],\n#     \"drop_path_rate\": [0.20, 0.60]\n# }\n# num_trials = np.prod([len(hpc) for hpc in hyperparams.values()])\n# print(\"num_trials = \",num_trials)\n# def build_model(hp):\n#     hp_kwargs = { key:hp.Choice(key, value, ordered=False) for key, value in hyperparams.items()}\n#     with strategy.scope():\n#         model = create_model(\n#                              pretrained_weights=pretrained_weights,\n#                              **hp_kwargs\n#                             )\n#     return model\n# tuner = keras_tuner.GridSearch(\n#     hypermodel=build_model,\n#     objective=keras_tuner.Objective('val_macro_F1', 'max'),\n#     max_trials=num_trials,\n#     seed=911,\n#     overwrite=False,\n#     directory=\"/kaggle/working\",\n#     project_name='keras-tuner',\n# )\n# tuner.search_space_summary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-26T12:51:10.351263Z","iopub.execute_input":"2024-06-26T12:51:10.351508Z","iopub.status.idle":"2024-06-26T12:51:10.362053Z","shell.execute_reply.started":"2024-06-26T12:51:10.351483Z","shell.execute_reply":"2024-06-26T12:51:10.361257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # D.model_var = \"convnext_base\"\n# # D.fusion_stage = 3\n# # D.fc_layers_depth = None\n# # D.fc_layers_dims = None\n# # D.dropout_rate = None\n# # D.pooling = None\n# fold = 1\n# try:\n#     with strategy.scope():\n#         model = create_model(model_var=D.model_var, \n#                              fusion_stage=D.fusion_stage,\n#                              fusion_block_index=D.fusion_index,\n#                              fc_layers_depth=D.fc_layers_depth, \n#                              fc_layers_dims=D.fc_layers_dims, \n#                              drop_path_rate=D.dropout_rate,\n#                              drop_out_rate=0.3,\n#                              pooling=D.pooling,\n#                              pretrained_weights=pretrained_weights,\n#                             )\n#         history = model_fit(model, \n#                             fold=fold,\n#                             initial_epoch=0,\n#                             epoch=D.epochs, \n#                             verbose = 2 if D.TPU else 1, \n#                             initial_val_F1=0.5\n#                            )\n#         model.load_weights(f\"checkpoints/{D.model_var}_fold-{fold}_best.weights.h5\")\n        \n# #         tuner.search(train_ds, \n# #                      epochs=D.epochs if D.TPU else 1, \n# #                      validation_data=validation_ds,\n# #                      verbose=2,\n# #                      callbacks=[get_lr_callback()]\n# #                     )\n# except IndexError as e:\n#     print(repr(e))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-26T12:51:10.362963Z","iopub.execute_input":"2024-06-26T12:51:10.363318Z","iopub.status.idle":"2024-06-26T12:51:10.381926Z","shell.execute_reply.started":"2024-06-26T12:51:10.363291Z","shell.execute_reply":"2024-06-26T12:51:10.381259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"histories = {}\n# models = []\nmodels_best_f1_score = []\nwith strategy.scope():\n    for fold in range(len(fold_ds)):\n        model = create_model(model_var=D.model_var, \n                         fusion_stage=D.fusion_stage,\n                         fusion_block_index=D.fusion_index,\n                         fc_layers_depth=D.fc_layers_depth, \n                         fc_layers_dims=D.fc_layers_dims, \n                         drop_path_rate=D.drop_path_rate,\n                         drop_out_rate=D.dropout_rate,\n                         pooling=D.pooling,\n                         pretrained_weights=pretrained_weights,\n                        )\n        print(f\"training model fold-{fold}\".center(40,\"-\"),\"\\nprevious fold result :\", models_best_f1_score)\n        history, best_f1_score = model_fit(model, \n                                            fold,\n                                            initial_epoch=0,\n                                            epoch=D.epochs, \n                                            verbose = 2 if D.TPU else 1, \n                                            initial_val_F1=0.5\n                                           )\n        histories[f\"fold-{fold}\"] = history\n        models_best_f1_score.append(best_f1_score)\n        model.load_weights(f\"checkpoints/{D.model_var}_fold-{fold}_best.weights.h5\")\n        model.save(f\"{D.model_var}_fold-{fold}_best-{best_f1_score:.3f}.h5\", include_optimizer=False)\n#         models.append(model)\n        clear_output()\nprint(f\"training finished\".center(40,\"-\"))\n!rm -rf checkpoints\nprint(\"F1-Score result :\")\nprint(\"\\n\".join([f\"fold-{i} = {score}\" for i, score in enumerate(models_best_f1_score)]))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T12:51:10.382846Z","iopub.execute_input":"2024-06-26T12:51:10.383190Z","iopub.status.idle":"2024-06-26T14:29:15.557437Z","shell.execute_reply.started":"2024-06-26T12:51:10.383164Z","shell.execute_reply":"2024-06-26T14:29:15.555955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result","metadata":{}},{"cell_type":"code","source":"# tuner.results_summary(num_trials)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T14:29:15.559277Z","iopub.execute_input":"2024-06-26T14:29:15.559637Z","iopub.status.idle":"2024-06-26T14:29:15.564678Z","shell.execute_reply.started":"2024-06-26T14:29:15.559602Z","shell.execute_reply":"2024-06-26T14:29:15.563862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history_metrics_for_multi_model(histories, metric_to_plot=\"macro_F1\")\nplot_history_metrics_for_multi_model(histories, metric_to_plot=\"val_macro_F1\")\nplot_history_metrics_for_multi_model(histories, metric_to_plot=\"accuracy\")\nplot_history_metrics_for_multi_model(histories, metric_to_plot=\"val_accuracy\")","metadata":{"execution":{"iopub.status.busy":"2024-06-26T14:29:15.565750Z","iopub.execute_input":"2024-06-26T14:29:15.566025Z","iopub.status.idle":"2024-06-26T14:29:16.471044Z","shell.execute_reply.started":"2024-06-26T14:29:15.565998Z","shell.execute_reply":"2024-06-26T14:29:16.470144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold, history in histories.items():\n    plot_history_metrics(history, model_name=fold)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T14:29:16.472043Z","iopub.execute_input":"2024-06-26T14:29:16.472322Z","iopub.status.idle":"2024-06-26T14:29:18.352830Z","shell.execute_reply.started":"2024-06-26T14:29:16.472295Z","shell.execute_reply":"2024-06-26T14:29:18.351916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# test the saved model","metadata":{}},{"cell_type":"code","source":"# with strategy.scope():\n#     modelll = create_model(model_var=D.model_var, \n#                              fusion_stage=D.fusion_stage,\n#                              fusion_block_index=0,\n#                              fc_layers_depth=D.fc_layers_depth, \n#                              fc_layers_dims=D.fc_layers_dims, \n#                              drop_path_rate=D.dropout_rate,\n#                              pooling=D.pooling,\n#                              pretrained_weights=None,\n#                             )\n#     modelll.load_weights(input(\"model weights path (.weights.h5) :\"))\n# #     modelll = keras.models.load_model(input(\"model path (.h5 or .keras) :\"))\n#     modelll.evaluate(validation_ds)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T14:29:18.354005Z","iopub.execute_input":"2024-06-26T14:29:18.354316Z","iopub.status.idle":"2024-06-26T14:29:18.358540Z","shell.execute_reply.started":"2024-06-26T14:29:18.354286Z","shell.execute_reply":"2024-06-26T14:29:18.357589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# modell = keras.models.load_model(\"/kaggle/working/convnext_dual_view.h5\")\n# modell.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-26T14:29:18.359498Z","iopub.execute_input":"2024-06-26T14:29:18.359739Z","iopub.status.idle":"2024-06-26T14:29:18.379259Z","shell.execute_reply.started":"2024-06-26T14:29:18.359714Z","shell.execute_reply":"2024-06-26T14:29:18.378530Z"},"trusted":true},"execution_count":null,"outputs":[]}]}